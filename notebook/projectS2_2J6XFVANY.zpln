{
  "paragraphs": [
    {
      "text": "%pyspark\nsc",
      "user": "anonymous",
      "dateUpdated": "2023-07-04T13:44:59+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://ede632b10975:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://spark:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>spark-shared_process</code></dd>\n            </dl>\n        </div>\n        "
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1688416856006_606304560",
      "id": "paragraph_1688416856006_606304560",
      "dateCreated": "2023-07-03T20:40:56+0000",
      "dateStarted": "2023-07-04T13:44:59+0000",
      "dateFinished": "2023-07-04T13:45:27+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:386"
    },
    {
      "text": "%pyspark\n!pip install spark-nlp",
      "user": "anonymous",
      "dateUpdated": "2023-07-03T20:47:49+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1688417111312_2090428415",
      "id": "paragraph_1688417111312_2090428415",
      "dateCreated": "2023-07-03T20:45:11+0000",
      "dateStarted": "2023-07-03T20:47:49+0000",
      "dateFinished": "2023-07-03T20:47:54+0000",
      "status": "FINISHED",
      "$$hashKey": "object:387"
    },
    {
      "text": "",
      "user": "anonymous",
      "dateUpdated": "2023-07-25T12:42:38+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1690288581025_753900678",
      "id": "paragraph_1690288581025_753900678",
      "dateCreated": "2023-07-25T12:36:21+0000",
      "status": "READY",
      "focus": true,
      "$$hashKey": "object:1549"
    },
    {
      "text": "%pyspark\nimport pandas as pd\nimport yfinance as yf\nfrom pyspark.sql import functions as F\n\n\ntopic=\"yfinance\"\n\ndata = yf.download(\"GOOGL\", start=\"2022-05-02\", end=\"2023-07-02\",)\ndata = data.reset_index()\n\ndata = spark.createDataFrame(data)\n\ndata = data.withColumn(\"value\", F.to_json(F.struct(*data.columns)))\n\n\ndata.write.format(\"kafka\")\\\n  .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\\\n  .option(\"topic\", topic) \\\n.save()\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-04T13:12:36+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\r[*********************100%***********************]  1 of 1 completed\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ede632b10975:4040/jobs/job?id=35",
              "$$hashKey": "object:894"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1688417269643_1215803221",
      "id": "paragraph_1688417269643_1215803221",
      "dateCreated": "2023-07-03T20:47:49+0000",
      "dateStarted": "2023-07-04T13:12:36+0000",
      "dateFinished": "2023-07-04T13:12:37+0000",
      "status": "FINISHED",
      "$$hashKey": "object:388"
    },
    {
      "text": "%pyspark\nfrom pyspark.sql.functions import from_json,col\nfrom pyspark.sql.types import StructType, StringType, DateType, DoubleType\n\n\nschema = StructType().add(\"Date\", DateType())\\\n                     .add(\"Open\", DoubleType())\\\n                     .add(\"High\", DoubleType())\\\n                     .add(\"Low\", DoubleType())\\\n                     .add(\"Close\", DoubleType())\\\n                     .add(\"Adj Close\", DoubleType())\\\n                     .add(\"Volume\", DoubleType())\n\n#Lire la donnée depuis le topic yfinance\ndata = spark \\\n            .read \\\n            .format(\"kafka\") \\\n            .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n            .option(\"subscribe\", \"yfinance\") \\\n            .load()\\\n            .selectExpr( \"CAST(value AS STRING)\")\\\n            .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n            .select(\"data.*\")\n    \n\n#Sauvegardé les données dans hdfs\n#data.write.parquet(\"hdfs://namenode:8020/hadoop/hdfs/yfdata/data.parquet\")\n\n#\ndata.show()",
      "user": "anonymous",
      "dateUpdated": "2023-07-04T13:45:35+0000",
      "progress": 100,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+----------+------------------+------------------+------------------+------------------+------------------+---------+\n|      Date|              Open|              High|               Low|             Close|         Adj Close|   Volume|\n+----------+------------------+------------------+------------------+------------------+------------------+---------+\n|2022-11-29|  95.7300033569336| 96.12000274658203| 94.11000061035156| 95.19000244140625| 95.19000244140625|2.00617E7|\n|2022-05-02|113.40499877929688|116.74549865722656| 112.5999984741211|116.58300018310547|116.58300018310547| 3.5534E7|\n|2022-11-30| 94.81999969482422|101.04000091552734| 94.41999816894531|100.98999786376953|100.98999786376953|4.36475E7|\n|2022-05-03|116.43049621582031|118.44200134277344|116.03450012207031|117.33399963378906|117.33399963378906| 2.4968E7|\n|2022-12-01| 101.0199966430664|            102.25|            100.25|100.98999786376953|100.98999786376953|2.86871E7|\n|2022-05-04|117.03150177001953|122.85449981689453|115.11599731445312|122.26100158691406|122.26100158691406| 4.9916E7|\n|2022-12-02| 99.05000305175781| 100.7699966430664|  98.9000015258789|100.44000244140625|100.44000244140625|2.14807E7|\n|2022-05-05| 120.2040023803711|121.03900146484375|115.00550079345703|116.50550079345703|116.50550079345703|  4.584E7|\n|2022-12-05|  99.4000015258789|101.37999725341797|              99.0|  99.4800033569336|  99.4800033569336|2.44051E7|\n|2022-05-06|115.18450164794922|117.57150268554688| 114.0155029296875|115.74649810791016|115.74649810791016|  3.971E7|\n|2022-12-06| 99.30000305175781| 99.77999877929688| 96.41999816894531|  96.9800033569336|  96.9800033569336|2.49107E7|\n|2022-05-09|            113.25| 115.0770034790039|112.00050354003906|112.51100158691406|112.51100158691406| 4.0802E7|\n|2022-12-07| 96.41000366210938| 96.87999725341797| 94.72000122070312| 94.94000244140625| 94.94000244140625|3.10454E7|\n|2022-05-10|115.50749969482422|            116.25|112.90049743652344| 114.3949966430664| 114.3949966430664|   3.99E7|\n|2022-12-08| 95.37999725341797| 95.58000183105469| 93.44999694824219| 93.70999908447266| 93.70999908447266|3.22133E7|\n|2022-05-11|113.23650360107422|  116.364501953125|113.23650360107422|113.60250091552734|113.60250091552734| 3.7534E7|\n|2022-12-09|  93.7699966430664| 94.26000213623047|             92.75| 92.83000183105469| 92.83000183105469|2.82254E7|\n|2022-05-12|111.37750244140625|114.29499816894531|109.82450103759766|112.84400177001953|112.84400177001953| 5.3836E7|\n|2022-12-12| 92.70999908447266| 93.55999755859375| 91.61000061035156| 93.30999755859375| 93.30999755859375|  2.942E7|\n|2022-05-13|114.53299713134766|           117.875| 113.6050033569336|116.05049896240234|116.05049896240234| 3.5038E7|\n+----------+------------------+------------------+------------------+------------------+------------------+---------+\nonly showing top 20 rows\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ede632b10975:4040/jobs/job?id=0",
              "$$hashKey": "object:940"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1688419237912_631408085",
      "id": "paragraph_1688419237912_631408085",
      "dateCreated": "2023-07-03T21:20:37+0000",
      "dateStarted": "2023-07-04T13:45:35+0000",
      "dateFinished": "2023-07-04T13:45:52+0000",
      "status": "FINISHED",
      "$$hashKey": "object:389"
    },
    {
      "text": "%pyspark\ndata.count()",
      "user": "anonymous",
      "dateUpdated": "2023-07-04T13:12:52+0000",
      "progress": 50,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "293"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ede632b10975:4040/jobs/job?id=37",
              "$$hashKey": "object:986"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1688427411376_1277177001",
      "id": "paragraph_1688427411376_1277177001",
      "dateCreated": "2023-07-03T23:36:51+0000",
      "dateStarted": "2023-07-04T13:12:52+0000",
      "dateFinished": "2023-07-04T13:12:56+0000",
      "status": "FINISHED",
      "$$hashKey": "object:390"
    },
    {
      "text": "%pyspark\nimport onnxruntime as rt\n\n# Chemin vers le fichier ONNX\nmodel_path ='/notebook/model1.onnx'\n\n# Charger le modèle ONNX\nsession = rt.InferenceSession(model_path)\ninputs = session.get_inputs()\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-04T13:46:02+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1688419240259_1962097529",
      "id": "paragraph_1688419240259_1962097529",
      "dateCreated": "2023-07-03T21:20:40+0000",
      "dateStarted": "2023-07-04T13:46:02+0000",
      "dateFinished": "2023-07-04T13:46:02+0000",
      "status": "FINISHED",
      "$$hashKey": "object:391"
    },
    {
      "text": "%pyspark\ndf = data.select(col('Close'))\n\n# Convert the DataFrame to a NumPy array\ndataset = df.rdd.map(lambda row: row[0]).collect()",
      "user": "anonymous",
      "dateUpdated": "2023-07-04T13:46:08+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ede632b10975:4040/jobs/job?id=1",
              "$$hashKey": "object:1072"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1688425438520_913115167",
      "id": "paragraph_1688425438520_913115167",
      "dateCreated": "2023-07-03T23:03:58+0000",
      "dateStarted": "2023-07-04T13:46:08+0000",
      "dateFinished": "2023-07-04T13:46:13+0000",
      "status": "FINISHED",
      "$$hashKey": "object:392"
    },
    {
      "text": "%pyspark\nfrom pyspark.ml.feature import VectorAssembler, MinMaxScaler\n\n# Convertir le tableau Numpy en DataFrame Spark\ndata_df = spark.createDataFrame([(float(value),) for value in dataset], [\"value\"])\n\n# Utiliser VectorAssembler pour regrouper les colonnes dans un vecteur\nassembler = VectorAssembler(inputCols=[\"value\"], outputCol=\"features\")\ndata_assembled = assembler.transform(data_df)\n\n# Instancier le MinMaxScaler\nscaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n\n# Calculer les statistiques de mise à l'échelle\nscaler_model = scaler.fit(data_assembled)\n\n# Appliquer la mise à l'échelle sur les données\nscaled_data_df = scaler_model.transform(data_assembled)\n\n# Extraire les données mises à l'échelle en tant que tableau Numpy\nscaled_data = scaled_data_df.select(\"scaled_features\").collect()",
      "user": "anonymous",
      "dateUpdated": "2023-07-04T13:46:15+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ede632b10975:4040/jobs/job?id=2",
              "$$hashKey": "object:1118"
            },
            {
              "jobUrl": "http://ede632b10975:4040/jobs/job?id=3",
              "$$hashKey": "object:1119"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1688425486841_1176274534",
      "id": "paragraph_1688425486841_1176274534",
      "dateCreated": "2023-07-03T23:04:46+0000",
      "dateStarted": "2023-07-04T13:46:15+0000",
      "dateFinished": "2023-07-04T13:46:19+0000",
      "status": "FINISHED",
      "$$hashKey": "object:393"
    },
    {
      "text": "%pyspark\nimport numpy as np\n\nfeature_length = 100\n# Convert the testing data to a Spark DataFrame\n#input_data = np.array(data.select(\"Close\").collect(), dtype=np.float32)\n\n# Convert scaled_data to a NumPy array\nscaled_data = np.array(scaled_data, dtype=np.float32)\n\n# Create a list of arrays containing the sliding windows\nx_test_arrays = [\n    scaled_data[i-feature_length:i, 0] for i in range(feature_length, len(scaled_data))\n]\n\nx_test_arrays = np.array(x_test_arrays)\nx_test = np.reshape(x_test_arrays, (x_test_arrays.shape[0], x_test_arrays.shape[1],1))\n\n\n# Effectuer les prédictions\npredictions = session.run(None, {\"x\": x_test})\n\n# Convertir la liste de valeurs prédites en un tableau NumPy\npredictions_array = np.array(predictions)\n\nXmax = data.select(\"Close\").rdd.max()[0]\nXmin = data.select(\"Close\").rdd.min()[0]\n\nprint(Xmax,Xmin)\n\nPred = ( Xmax - Xmin) * predictions_array + Xmin",
      "user": "anonymous",
      "dateUpdated": "2023-07-04T13:46:32+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "127.30999755859375 83.43000030517578\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ede632b10975:4040/jobs/job?id=4",
              "$$hashKey": "object:1169"
            },
            {
              "jobUrl": "http://ede632b10975:4040/jobs/job?id=5",
              "$$hashKey": "object:1170"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1688425451506_1490716030",
      "id": "paragraph_1688425451506_1490716030",
      "dateCreated": "2023-07-03T23:04:11+0000",
      "dateStarted": "2023-07-04T13:46:32+0000",
      "dateFinished": "2023-07-04T13:46:41+0000",
      "status": "FINISHED",
      "$$hashKey": "object:394"
    },
    {
      "text": "%pyspark\nlen(Pred[0])",
      "user": "anonymous",
      "dateUpdated": "2023-07-04T13:13:45+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "193"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1688475603766_581706980",
      "id": "paragraph_1688475603766_581706980",
      "dateCreated": "2023-07-04T13:00:03+0000",
      "dateStarted": "2023-07-04T13:13:45+0000",
      "dateFinished": "2023-07-04T13:13:46+0000",
      "status": "FINISHED",
      "$$hashKey": "object:395"
    },
    {
      "text": "%pyspark\nimport pandas as pd\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.functions import desc\nfrom pyspark.sql.functions import lit\n\n\n# Convert the Pred array to a Pandas DataFrame\npred_df = pd.DataFrame({'Pred': Pred.flatten()})\n\npred_spark_df = spark.createDataFrame(pred_df)\n\n# Get the last 193 rows from the Date and Close columns of the original data DataFrame\nlast_data = data.select(\"Date\", \"Close\").orderBy(desc(\"Date\")).limit(193)\n\n# Convert the last_data Spark DataFrame to a Pandas DataFrame\nlast_data_pd = last_data.toPandas()\n\n# Merge last_data_pd with pred_df\nmerged_df = pd.merge(last_data_pd, pred_df, left_index=True, right_index=True)\n\n\n# Create a new Spark DataFrame from the merged Pandas DataFrame\nnew_df = spark.createDataFrame(merged_df)\n\n# Verify the new Spark DataFrame\nnew_df.show()\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-04T13:46:52+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+----------+------------------+------------------+\n|      Date|             Close|              Pred|\n+----------+------------------+------------------+\n|2023-06-30|119.69999694824219| 95.28124237060547|\n|2023-06-29| 119.0999984741211| 98.64263916015625|\n|2023-06-28|120.18000030517578|101.00557708740234|\n|2023-06-27|118.33000183105469|   101.71044921875|\n|2023-06-26|118.33999633789062|103.77949523925781|\n|2023-06-23|122.33999633789062|104.22335815429688|\n|2023-06-22| 123.1500015258789| 105.3472900390625|\n|2023-06-21|120.55000305175781|110.70507049560547|\n|2023-06-20| 123.0999984741211|108.92279052734375|\n|2023-06-16|123.52999877929688|105.83007049560547|\n|2023-06-15|125.08999633789062|  108.509521484375|\n|2023-06-14|123.66999816894531|105.62297821044922|\n|2023-06-13|123.83000183105469|105.98035430908203|\n|2023-06-12|123.63999938964844|104.11451721191406|\n|2023-06-09| 122.2300033569336|105.48551177978516|\n|2023-06-08|122.13999938964844|103.62767791748047|\n|2023-06-07|             122.5|103.60798645019531|\n|2023-06-06|127.30999755859375|104.22624206542969|\n|2023-06-05|126.01000213623047|104.75572204589844|\n|2023-06-02|124.66999816894531| 104.8377685546875|\n+----------+------------------+------------------+\nonly showing top 20 rows\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ede632b10975:4040/jobs/job?id=6",
              "$$hashKey": "object:1260"
            },
            {
              "jobUrl": "http://ede632b10975:4040/jobs/job?id=7",
              "$$hashKey": "object:1261"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1688425452547_57455489",
      "id": "paragraph_1688425452547_57455489",
      "dateCreated": "2023-07-03T23:04:12+0000",
      "dateStarted": "2023-07-04T13:46:52+0000",
      "dateFinished": "2023-07-04T13:46:58+0000",
      "status": "FINISHED",
      "$$hashKey": "object:396"
    },
    {
      "text": "%pyspark\nmerged_df.set_index(\"Date\", inplace = True)\nmerged_df.head()",
      "user": "anonymous",
      "dateUpdated": "2023-07-04T13:47:03+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Close</th>\n      <th>Pred</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2023-06-30</th>\n      <td>119.699997</td>\n      <td>95.281242</td>\n    </tr>\n    <tr>\n      <th>2023-06-29</th>\n      <td>119.099998</td>\n      <td>98.642639</td>\n    </tr>\n    <tr>\n      <th>2023-06-28</th>\n      <td>120.180000</td>\n      <td>101.005577</td>\n    </tr>\n    <tr>\n      <th>2023-06-27</th>\n      <td>118.330002</td>\n      <td>101.710449</td>\n    </tr>\n    <tr>\n      <th>2023-06-26</th>\n      <td>118.339996</td>\n      <td>103.779495</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1688476573531_804458281",
      "id": "paragraph_1688476573531_804458281",
      "dateCreated": "2023-07-04T13:16:13+0000",
      "dateStarted": "2023-07-04T13:47:03+0000",
      "dateFinished": "2023-07-04T13:47:03+0000",
      "status": "FINISHED",
      "$$hashKey": "object:397"
    },
    {
      "text": "%pyspark\nfrom elasticsearch import Elasticsearch\n\n# Configurer la connexion Elasticsearch\nes = Elasticsearch(['http://elasticsearch:9200']) \n\n# Parcourir les données et les indexer dans Elasticsearch\nfor index, row in merged_df.iterrows():\n    document = {\n        'symbol': 'GOOGL',\n        'date': index.strftime('%Y-%m-%d'),\n        'price': row['Close'],\n        'predicted': row['Pred']\n    }\n    es.index(index='google_prices', body=document)\n\n# Vérifier si les données ont été indexées avec succès\nres = es.search(index='google_prices', q='symbol:GOOGL')\nfor hit in res['hits']['hits']:\n    print(hit['_source'])\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-04T13:47:45+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "/opt/conda/envs/python_3_with_R/lib/python3.7/site-packages/ipykernel_launcher.py:14: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use the 'document' parameter. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n  \n/opt/conda/envs/python_3_with_R/lib/python3.7/site-packages/ipykernel_launcher.py:14: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.16/security-minimal-setup.html to enable security.\n  \n{'symbol': 'GOOGL', 'date': '2023-06-30', 'price': 119.69999694824219, 'predicted': 95.28124237060547}\n{'symbol': 'GOOGL', 'date': '2023-06-29', 'price': 119.0999984741211, 'predicted': 98.64263916015625}\n{'symbol': 'GOOGL', 'date': '2023-06-28', 'price': 120.18000030517578, 'predicted': 101.00557708740234}\n{'symbol': 'GOOGL', 'date': '2023-06-27', 'price': 118.33000183105469, 'predicted': 101.71044921875}\n{'symbol': 'GOOGL', 'date': '2023-06-26', 'price': 118.33999633789062, 'predicted': 103.77949523925781}\n{'symbol': 'GOOGL', 'date': '2023-06-23', 'price': 122.33999633789062, 'predicted': 104.22335815429688}\n{'symbol': 'GOOGL', 'date': '2023-06-22', 'price': 123.1500015258789, 'predicted': 105.3472900390625}\n{'symbol': 'GOOGL', 'date': '2023-06-21', 'price': 120.55000305175781, 'predicted': 110.70507049560547}\n{'symbol': 'GOOGL', 'date': '2023-06-20', 'price': 123.0999984741211, 'predicted': 108.92279052734375}\n{'symbol': 'GOOGL', 'date': '2023-06-16', 'price': 123.52999877929688, 'predicted': 105.83007049560547}\n/opt/conda/envs/python_3_with_R/lib/python3.7/site-packages/ipykernel_launcher.py:17: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.16/security-minimal-setup.html to enable security.\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1688428043667_734220497",
      "id": "paragraph_1688428043667_734220497",
      "dateCreated": "2023-07-03T23:47:23+0000",
      "dateStarted": "2023-07-04T13:47:45+0000",
      "dateFinished": "2023-07-04T13:47:56+0000",
      "status": "FINISHED",
      "$$hashKey": "object:398"
    },
    {
      "text": "%pyspark\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-04T00:20:10+0000",
      "progress": 0,
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1688430010755_1346975231",
      "id": "paragraph_1688430010755_1346975231",
      "dateCreated": "2023-07-04T00:20:10+0000",
      "status": "READY",
      "$$hashKey": "object:399"
    }
  ],
  "name": "projectS2",
  "id": "2J6XFVANY",
  "defaultInterpreterGroup": "spark",
  "version": "0.10.1",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/projectS2"
}